{
	"name": "Load_to_warehouse",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ghparkcluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "111544df-a08c-4a8d-81b0-3ba81c5bdde5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/56fbbfce-6dab-4d62-af16-ccd107f4d9d3/resourceGroups/DEV-EA-EASTUS-DF-RG/providers/Microsoft.Synapse/workspaces/healthdataws/bigDataPools/ghparkcluster",
				"name": "ghparkcluster",
				"type": "Spark",
				"endpoint": "https://healthdataws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ghparkcluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.types import IntegerType, DoubleType\n",
					"from pyspark.sql.window import Window\n",
					"import pandas as pd\n",
					"import statsmodels.api as sm\n",
					"\n",
					"schema_name = 'warehouse'\n",
					"table_name = 'uk_health_data'\n",
					"# Read from staging\n",
					"resultDF = spark.sql('SELECT year,health_exp as health_exp_percentage_gdp,life_expect, maternal_mortality,infant_mortality,neonatal_mortality, under_5_mortality   FROM `default`.`worldhealthdata` where country_code = \"GBR\" and year > 1999 and  year < 2023')\n",
					"\n",
					"# Create schema if does not exists \n",
					"spark_sql_schema_Create = f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\"\n",
					"spark.sql(spark_sql_schema_Create)\n",
					"# Write as delta \n",
					"resultDF.write.format(\"delta\").mode(\"ignore\").saveAsTable(f\"{schema_name}.{table_name}\")\n",
					"# Select data \n",
					"df = spark.sql(\"SELECT * FROM `warehouse`.`uk_health_data`\")\n",
					"\n",
					"\n",
					"# Fill null values for forecasting (simplest approach, consider more sophisticated methods)\n",
					"# Fill null values for forecasting (simplest approach, consider more sophisticated methods)\n",
					"columns = [\"year\", \"health_exp_percentage_gdp\", \"life_expect\", \"maternal_mortality\", \"infant_mortality\", \"neonatal_mortality\", \"under_5_mortality\"]\n",
					"\n",
					"# Convert 'year' column to integer\n",
					"df = df.withColumn(\"year\", df[\"year\"].cast(IntegerType()))\n",
					"\n",
					"# Convert all other columns to double\n",
					"for col in columns[1:]:\n",
					"    df = df.withColumn(col, F.col(col).cast(DoubleType()))\n",
					"\n",
					"# Fill null values using window function\n",
					"for col in columns[1:]:\n",
					"    window_spec = Window.orderBy(\"year\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
					"    df = df.withColumn(col, F.last(F.col(col), ignorenulls=True).over(window_spec))\n",
					"\n",
					"\n",
					"df.show(24)\n",
					"\n",
					"\n",
					"\n",
					"# Forecasting function\n",
					"def forecast_column(pdf, column_name, forecast_years=5):\n",
					"    pdf = pdf.sort_values(by=\"year\")\n",
					"    time_series = pd.Series(pdf[column_name].values, index=pd.to_datetime(pdf[\"year\"], format='%Y'))\n",
					"    time_series.index.freq = 'YS' #Set the frequency after creation\n",
					"\n",
					"    try:\n",
					"        model = sm.tsa.ARIMA(time_series, order=(1, 1, 0)) # Example order\n",
					"        model_fit = model.fit()\n",
					"        forecast = model_fit.forecast(steps=forecast_years)\n",
					"        forecast_df = pd.DataFrame({\"year\": forecast.index.year, f\"{column_name}_forecast\": forecast.values})\n",
					"        return forecast_df\n",
					"    except Exception as e:\n",
					"        print(f\"Error forecasting {column_name}: {e}\")\n",
					"        return pd.DataFrame()\n",
					"\n",
					"# Forecast each column\n",
					"forecast_results = None\n",
					"forecast_years = 5\n",
					"start_year = df.agg(F.max(\"year\")).collect()[0][0] + 1\n",
					"forecast_years_range = range(start_year, start_year + forecast_years)\n",
					"\n",
					"for column in columns[1:]:  # Skip 'year'\n",
					"    result = df.toPandas() #convert to pandas to then be able to pass to forecast_column\n",
					"    forecast = forecast_column(result, column, forecast_years)\n",
					"\n",
					"    if forecast is not None and not forecast.empty:\n",
					"        if forecast_results is None:\n",
					"            forecast_results = forecast\n",
					"        else:\n",
					"            forecast_results = pd.merge(forecast_results, forecast, on=\"year\", how=\"outer\")\n",
					"\n",
					"# Convert back to PySpark DataFrame\n",
					"if forecast_results is not None:\n",
					"    forecast_spark_df = spark.createDataFrame(forecast_results)\n",
					"    forecast_spark_df.show()"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.types import IntegerType, DoubleType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.ml.feature import VectorAssembler\n",
					"from pyspark.ml.regression import GBTRegressor\n",
					"from pyspark.ml.evaluation import RegressionEvaluator\n",
					"from pyspark.ml import Pipeline\n",
					"\n",
					"schema_name = 'warehouse'\n",
					"table_name = 'uk_health_data'\n",
					"# Read from staging\n",
					"resultDF = spark.sql('SELECT year,health_exp as health_exp_percentage_gdp,life_expect, maternal_mortality,infant_mortality,neonatal_mortality, under_5_mortality   FROM `default`.`worldhealthdata` where country_code = \"GBR\" and year > 1999 and  year < 2023')\n",
					"\n",
					"# Create schema if does not exists \n",
					"spark_sql_schema_Create = f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\"\n",
					"spark.sql(spark_sql_schema_Create)\n",
					"# Write as delta \n",
					"resultDF.write.format(\"delta\").mode(\"ignore\").saveAsTable(f\"{schema_name}.{table_name}\")\n",
					"# Select data \n",
					"df = spark.sql(\"SELECT * FROM `warehouse`.`uk_health_data`\")\n",
					"\n",
					"# Fill null values for forecasting (simplest approach, consider more sophisticated methods)\n",
					"columns = [\"year\", \"health_exp_percentage_gdp\", \"life_expect\", \"maternal_mortality\", \"infant_mortality\", \"neonatal_mortality\", \"under_5_mortality\"]\n",
					"\n",
					"# Convert 'year' column to integer\n",
					"df = df.withColumn(\"year\", df[\"year\"].cast(IntegerType()))\n",
					"\n",
					"# Convert all other columns to double\n",
					"for col in columns[1:]:\n",
					"    df = df.withColumn(col, F.col(col).cast(DoubleType()))\n",
					"\n",
					"# Fill null values using window function\n",
					"for col in columns[1:]:\n",
					"    window_spec = Window.orderBy(\"year\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
					"    df = df.withColumn(col, F.last(F.col(col), ignorenulls=True).over(window_spec))\n",
					"\n",
					"\n",
					"# Feature Engineering (Lagged Features)\n",
					"lag_window = Window.orderBy(\"year\")\n",
					"\n",
					"for col in columns[1:]:\n",
					"    df = df.withColumn(f\"{col}_lag1\", F.lag(col).over(lag_window))\n",
					"\n",
					"# Explicitly cast lagged features to DoubleType\n",
					"for col in columns[1:]:\n",
					"    df = df.withColumn(f\"{col}_lag1\", F.col(f\"{col}_lag1\").cast(DoubleType()))\n",
					"\n",
					"# Fill nulls created by lag function.\n",
					"for col in columns[1:]:\n",
					"    df = df.fillna({f\"{col}_lag1\": 0})\n",
					"\n",
					"\n",
					"# Prepare data for GBTRegressor\n",
					"forecast_columns = columns[1:]\n",
					"forecast_results = {}\n",
					"\n",
					"# Create feature_cols list before the loop\n",
					"feature_cols = [f\"{col}_lag1\" for col in forecast_columns]\n",
					"\n",
					"assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
					"df = assembler.transform(df)\n",
					"\n",
					"\n",
					"for col in forecast_columns:\n",
					"    label_col = col\n",
					"\n",
					"    gbt = GBTRegressor(labelCol=label_col, featuresCol=\"features\", maxIter=10)\n",
					"    pipeline = Pipeline(stages=[gbt])\n",
					"\n",
					"    max_year = df.agg(F.max(\"year\")).collect()[0][0]\n",
					"    train_df = df.filter(df.year < max_year - 2)\n",
					"    test_df = df.filter(df.year >= max_year - 2)\n",
					"\n",
					"    model = pipeline.fit(train_df)\n",
					"    predictions = model.transform(test_df)\n",
					"\n",
					"    evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
					"    rmse = evaluator.evaluate(predictions)\n",
					"    print(f\"RMSE for {label_col}: {rmse}\")\n",
					"\n",
					"    # Forecast for the next 3 years\n",
					"    last_year = max_year\n",
					"    forecast_data = []\n",
					"\n",
					"\n",
					"    # Get last values for all lagged columns\n",
					"    last_row = test_df.select(*feature_cols).collect()[-1]\n",
					"    last_values = [row for row in last_row]\n",
					"\n",
					"    for i in range(1, 4):\n",
					"        forecast_year = last_year + i\n",
					"        forecast_df = spark.createDataFrame([last_values], feature_cols) # Corrected line\n",
					"        forecast_df = forecast_df.withColumnRenamed(col, f\"{col}_lag1\")\n",
					"        forecast_prediction = model.transform(assembler.transform(forecast_df)).select(\"prediction\").collect()[0][0]\n",
					"        forecast_data.append((forecast_year, forecast_prediction))\n",
					"        last_val = forecast_prediction\n",
					"\n",
					"    forecast_results[col] = forecast_data\n",
					"\n",
					"for col, forecast_data in forecast_results.items():\n",
					"    print(f\"Forecasts for {col}:\")\n",
					"    for year, prediction in forecast_data:\n",
					"        print(f\"  Year: {year}, Prediction: {prediction}\")"
				],
				"execution_count": 38
			}
		]
	}
}