{
	"name": "forecasting",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ghparkcluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "53449c69-5e09-439d-8cdf-9cf10eb2f0c9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/56fbbfce-6dab-4d62-af16-ccd107f4d9d3/resourceGroups/DEV-EA-EASTUS-DF-RG/providers/Microsoft.Synapse/workspaces/healthdataws/bigDataPools/ghparkcluster",
				"name": "ghparkcluster",
				"type": "Spark",
				"endpoint": "https://healthdataws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ghparkcluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## ARIMA"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.types import IntegerType, DoubleType\n",
					"from pyspark.sql.window import Window\n",
					"import pandas as pd\n",
					"import statsmodels.api as sm\n",
					"\n",
					"schema_name = 'warehouse'\n",
					"table_name = 'uk_health_data'\n",
					"table_name_forecast = 'uk_health_data_forecast'\n",
					"\n",
					"df = spark.sql(\"SELECT * FROM `warehouse`.`uk_health_data`\")\n",
					"\n",
					"# Fill null values for forecasting (simplest approach, consider more sophisticated methods)\n",
					"columns = [\"year\", \"health_exp_percentage_gdp\", \"life_expect\", \"maternal_mortality\", \"infant_mortality\", \"neonatal_mortality\", \"under_5_mortality\"]\n",
					"\n",
					"# Convert 'year' column to integer\n",
					"#df = df.withColumn(\"year\", df[\"year\"].cast(IntegerType()))\n",
					"\n",
					"# Convert all other columns to double\n",
					"#for col in columns[1:]:\n",
					"#    df = df.withColumn(col, F.col(col).cast(DoubleType()))\n",
					"\n",
					"# Fill null values using window function\n",
					"#for col in columns[1:]:\n",
					"#    window_spec = Window.orderBy(\"year\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
					"#    df = df.withColumn(col, F.last(F.col(col), ignorenulls=True).over(window_spec))\n",
					"\n",
					"\n",
					"df.show(24)\n",
					"\n",
					"#resultDF.write.format(\"delta\").mode(\"ignore\").saveAsTable(f\"{schema_name}.{table_name_appended}\")\n",
					"\n",
					"\n",
					"\n",
					"# Forecasting function\n",
					"def forecast_column(pdf, column_name, forecast_years=5):\n",
					"    pdf = pdf.sort_values(by=\"year\")\n",
					"    time_series = pd.Series(pdf[column_name].values, index=pd.to_datetime(pdf[\"year\"], format='%Y'))\n",
					"    time_series.index.freq = 'YS' #Set the frequency after creation\n",
					"\n",
					"    try:\n",
					"        model = sm.tsa.ARIMA(time_series, order=(1, 1, 1)) # Example order\n",
					"        model_fit = model.fit()\n",
					"        forecast = model_fit.forecast(steps=forecast_years)\n",
					"        forecast_df = pd.DataFrame({\"year\": forecast.index.year, f\"{column_name}_forecast\": forecast.values})\n",
					"        return forecast_df\n",
					"    except Exception as e:\n",
					"        print(f\"Error forecasting {column_name}: {e}\")\n",
					"        return pd.DataFrame()\n",
					"\n",
					"# Forecast each column\n",
					"forecast_results = None\n",
					"forecast_years = 5\n",
					"start_year = df.agg(F.max(\"year\")).collect()[0][0] + 1\n",
					"forecast_years_range = range(start_year, start_year + forecast_years)\n",
					"\n",
					"for column in columns[1:]:  # Skip 'year'\n",
					"    result = df.toPandas() #convert to pandas to then be able to pass to forecast_column\n",
					"    forecast = forecast_column(result, column, forecast_years)\n",
					"\n",
					"    if forecast is not None and not forecast.empty:\n",
					"        if forecast_results is None:\n",
					"            forecast_results = forecast\n",
					"        else:\n",
					"            forecast_results = pd.merge(forecast_results, forecast, on=\"year\", how=\"outer\")\n",
					"\n",
					"# Convert back to PySpark DataFrame\n",
					"if forecast_results is not None:\n",
					"    forecast_spark_df = spark.createDataFrame(forecast_results)\n",
					"    forecast_spark_df.show()\n",
					"\n",
					"forecast_spark_df.write.format(\"delta\").mode(\"ignore\").saveAsTable(f\"{schema_name}.{table_name_forecast}\")"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Machine Learning"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.types import IntegerType, DoubleType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.ml.feature import VectorAssembler\n",
					"from pyspark.ml.regression import GBTRegressor\n",
					"from pyspark.ml.evaluation import RegressionEvaluator\n",
					"from pyspark.ml import Pipeline\n",
					"\n",
					"schema_name = 'warehouse'\n",
					"table_name = 'uk_health_data'\n",
					"\n",
					"# Select data \n",
					"df = spark.sql(\"SELECT * FROM `warehouse`.`uk_health_data`\")\n",
					"\n",
					"df.show(24)\n",
					"\n",
					"# Fill null values for forecasting (simplest approach, consider more sophisticated methods)\n",
					"columns = [\"year\", \"health_exp_percentage_gdp\", \"life_expect\", \"maternal_mortality\", \"infant_mortality\", \"neonatal_mortality\", \"under_5_mortality\"]\n",
					"\n",
					"\n",
					"# Feature Engineering (Lagged Features)\n",
					"lag_window = Window.orderBy(\"year\")\n",
					"\n",
					"for col in columns[1:]:\n",
					"    df = df.withColumn(f\"{col}_lag1\", F.lag(col).over(lag_window))\n",
					"    df = df.withColumn(f\"{col}_lag2\", F.lag(col, 2).over(lag_window))\n",
					"    df = df.withColumn(f\"{col}_lag3\", F.lag(col, 3).over(lag_window))\n",
					"\n",
					"# Explicitly cast lagged features to DoubleType\n",
					"for col in columns[1:]:\n",
					"    for lag in [\"1\", \"2\", \"3\"]:\n",
					"        df = df.withColumn(f\"{col}_lag{lag}\", F.col(f\"{col}_lag{lag}\").cast(DoubleType()))\n",
					"\n",
					"\n",
					"# Fill nulls created by lag function.\n",
					"for col in columns[1:]:\n",
					"    for lag in [\"1\", \"2\", \"3\"]:\n",
					"        df = df.fillna({f\"{col}_lag{lag}\": 0})\n",
					"\n",
					"# Prepare data for GBTRegressor\n",
					"forecast_columns = columns[1:]\n",
					"forecast_results = {}\n",
					"\n",
					"# Create feature_cols list before the loop\n",
					"#feature_cols = [f\"{col}_lag{lag}\" for col in forecast_columns for lag in [\"1\", \"2\", \"3\"]] + [\"year\"]\n",
					"\n",
					"\n",
					"#assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
					"#df = assembler.transform(df)\n",
					"\n",
					"\n",
					"for col in forecast_columns:\n",
					"    label_col = col\n",
					"\n",
					"    # Create feature_cols list inside the loop\n",
					"    feature_cols = [f\"{col}_lag{lag}\" for col in forecast_columns for lag in [\"1\", \"2\", \"3\"]] + [\"year\"]\n",
					"\n",
					"   # Create VectorAssembler inside the loop\n",
					"    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
					"\n",
					"    max_year = df.agg(F.max(\"year\")).collect()[0][0]\n",
					"    train_df = df.filter(df.year < max_year - 5)\n",
					"    test_df = df.filter(df.year >= max_year - 5)\n",
					"\n",
					"    # Apply assembler to train and test data.\n",
					"    train_df = assembler.transform(train_df)\n",
					"    test_df = assembler.transform(test_df)\n",
					"\n",
					"    \n",
					"\n",
					"    gbt = GBTRegressor(labelCol=label_col, featuresCol=\"features\", maxIter=10)\n",
					"    pipeline = Pipeline(stages=[gbt])\n",
					"\n",
					"\n",
					"    model = pipeline.fit(train_df)\n",
					"    predictions = model.transform(test_df)\n",
					"\n",
					"    evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
					"    rmse = evaluator.evaluate(predictions)\n",
					"    print(f\"RMSE for {label_col}: {rmse}\")\n",
					"\n",
					"    # Forecast for the next 3 years\n",
					"    last_year = max_year\n",
					"    forecast_data = []\n",
					"\n",
					"    # Initialize last_values with dummy values\n",
					"    last_values = [0.0] * len(feature_cols)\n",
					"    \n",
					"\n",
					"\n",
					"    # Get last values for all lagged columns from test_df for the first forecast\n",
					"    test_last_row = test_df.select(*feature_cols).collect()[-1]\n",
					"    test_last_values = [float(test_last_row[feature]) if 'lag' in feature else int(test_last_row[feature]) for feature in feature_cols]\n",
					"\n",
					"\n",
					"      #Debug information.\n",
					"    print(f\"feature_cols: {feature_cols}\")\n",
					"    print(f\"test_last_values: {test_last_values}\")\n",
					"\n",
					"    for i in range(1, 6):\n",
					"        forecast_year = last_year + i\n",
					"        if i == 1:\n",
					"            #Explicitly create dataframe with correct column order.\n",
					"            forecast_df = spark.createDataFrame([test_last_values], feature_cols)\n",
					"        else:\n",
					"            forecast_df = spark.createDataFrame([last_values], feature_cols)\n",
					"\n",
					"        forecast_prediction = model.transform(assembler.transform(forecast_df)).select(\"prediction\").collect()[0][0]\n",
					"        forecast_data.append((forecast_year, forecast_prediction))\n",
					"\n",
					"        # Correctly update last_values\n",
					"        new_last_values = last_values[3:] + [forecast_prediction, forecast_year]\n",
					"        last_values = new_last_values\n",
					"     \n",
					"        for j in range(3):\n",
					"            new_last_values.append(last_values[j + 1])\n",
					"        new_last_values.append(forecast_prediction)\n",
					"        new_last_values.append(forecast_year)\n",
					"        last_values = new_last_values\n",
					"\n",
					"    forecast_results[col] = forecast_data\n",
					"\n",
					"for col, forecast_data in forecast_results.items():\n",
					"    print(f\"Forecasts for {col}:\")\n",
					"    for year, prediction in forecast_data:\n",
					"        print(f\"  Year: {year}, Prediction: {prediction}\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"\n",
					"import pandas as pd\n",
					"from sklearn.linear_model import LinearRegression\n",
					"import numpy as np\n",
					"\n",
					"\n",
					"df1 = spark.sql(\"SELECT * FROM `warehouse`.`uk_health_data`\")\n",
					"\n",
					"df1.show()\n",
					"\n",
					"# Load data from the Spark SQL table\n",
					"df = df1.toPandas()\n",
					"\n",
					"print(df)\n",
					"\n",
					"# Fill missing values with the mean of the column\n",
					"df = df.fillna(df.mean())\n",
					"\n",
					"# Features and target variables\n",
					"X = df[['year']]\n",
					"y_health_exp = df['health_exp_percentage_gdp']\n",
					"y_life_expect = df['life_expect']\n",
					"y_maternal_mortality = df['maternal_mortality']\n",
					"y_infant_mortality = df['infant_mortality']\n",
					"y_neonatal_mortality = df['neonatal_mortality']\n",
					"y_under_5_mortality = df['under_5_mortality']\n",
					"\n",
					"# Create and train the model for each target variable\n",
					"model_health_exp = LinearRegression().fit(X, y_health_exp)\n",
					"model_life_expect = LinearRegression().fit(X, y_life_expect)\n",
					"model_maternal_mortality = LinearRegression().fit(X, y_maternal_mortality)\n",
					"model_infant_mortality = LinearRegression().fit(X, y_infant_mortality)\n",
					"model_neonatal_mortality = LinearRegression().fit(X, y_neonatal_mortality)\n",
					"model_under_5_mortality = LinearRegression().fit(X, y_under_5_mortality)\n",
					"\n",
					"# Predict for the next five years\n",
					"future_years = np.array([[2023], [2024], [2025], [2026], [2027]])\n",
					"\n",
					"pred_health_exp = model_health_exp.predict(future_years)\n",
					"pred_life_expect = model_life_expect.predict(future_years)\n",
					"pred_maternal_mortality = model_maternal_mortality.predict(future_years)\n",
					"pred_infant_mortality = model_infant_mortality.predict(future_years)\n",
					"pred_neonatal_mortality = model_neonatal_mortality.predict(future_years)\n",
					"pred_under_5_mortality = model_under_5_mortality.predict(future_years)\n",
					"\n",
					"# Print predictions\n",
					"for i in range(len(future_years)):\n",
					"    print(f\"Year: {future_years[i][0]}\")\n",
					"    print(f\"Health Expenditure (% GDP): {pred_health_exp[i]}\")\n",
					"    print(f\"Life Expectancy: {pred_life_expect[i]}\")\n",
					"    print(f\"Maternal Mortality: {pred_maternal_mortality[i]}\")\n",
					"    print(f\"Infant Mortality: {pred_infant_mortality[i]}\")\n",
					"    print(f\"Neonatal Mortality: {pred_neonatal_mortality[i]}\")\n",
					"    print(f\"Under-5 Mortality: {pred_under_5_mortality[i]}\")\n",
					"    print(\"\\n\")\n",
					""
				],
				"execution_count": 9
			}
		]
	}
}