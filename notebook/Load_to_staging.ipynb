{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data from sources to staging tables"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load files to staging tables from Azure object/blob store\r\n",
        "## If it already exists, then it skips\r\n",
        "## It then stores into deleta table in parquet format "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Define the storage account and container\r\n",
        "storage_account_name = \"distributedanalytics\"\r\n",
        "container_name = \"healthdata-fs\"\r\n",
        "directory_path = \"data\"\r\n",
        "\r\n",
        "# Define the full path in which raw CSVs are placed\r\n",
        "full_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{directory_path}\"\r\n",
        "\r\n",
        "\r\n",
        "fs  = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\r\n",
        "\r\n",
        "# List files\r\n",
        "path = sc._jvm.org.apache.hadoop.fs.Path(full_path)\r\n",
        "files = fs.listStatus(path)\r\n",
        "# Display the files\r\n",
        "for file in files:\r\n",
        "    file_name = file.getPath().getName()\r\n",
        "    if file_name.endswith(\".csv\"):\r\n",
        "        # Read the CSV file into a DataFrame\r\n",
        "        print(f\"the file is {full_path}/{file_name}\")\r\n",
        "        df = spark.read.option(\"header\", \"true\").csv(f\"{full_path}/{file_name}\")\r\n",
        "\r\n",
        "\r\n",
        "        # Define the Delta Lake table path\r\n",
        "        delta_table_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/delta/{file_name.replace('.csv', '')}\"\r\n",
        "         # Check if the Delta table already exists\r\n",
        "        if not fs.exists(sc._jvm.org.apache.hadoop.fs.Path(delta_table_path)):\r\n",
        "            # Read the CSV file into a DataFrame\r\n",
        "            df = spark.read.option(\"header\", \"true\").csv(f\"{full_path}/{file_name}\")\r\n",
        "            df.write.format(\"delta\").mode(\"overwrite\").option(\"delta.columnMapping.mode\", \"name\").save(delta_table_path)\r\n",
        "            delta_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
        "\r\n",
        "\r\n",
        "            table_name = file_name.replace('.csv', '')\r\n",
        "            # Write data to Lake database table\r\n",
        "            delta_df.write.format(\"parquet\").saveAsTable(f\"default.{table_name}\")\r\n",
        "\r\n",
        "\r\n",
        "            print(f\"Delta Lake table created for {file_name} at {delta_table_path}\")\r\n",
        "        else:\r\n",
        "            print(f\"Delta Lake table already exists for {file_name}, skipping...\")\r\n",
        "            \r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ghparkcluster",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "34",
              "normalized_state": "finished",
              "queued_time": "2025-03-17T16:18:40.2815915Z",
              "session_start_time": "2025-03-17T16:18:40.2829824Z",
              "execution_start_time": "2025-03-17T16:23:46.772071Z",
              "execution_finish_time": "2025-03-17T16:24:23.258661Z",
              "parent_msg_id": "9ef06496-ffc9-4138-9f00-54ce9d6ca4f8"
            },
            "text/plain": "StatementMeta(ghparkcluster, 34, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/CHEAspercentGDP.csv\nDelta Lake table already exists for CHEAspercentGDP.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/CHEInMillionConstant2022USD.csv\nDelta Lake table already exists for CHEInMillionConstant2022USD.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/CHEPerCapitaUSD.csv\nDelta Lake table already exists for CHEPerCapitaUSD.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/DoctorsUK.csv\nDelta Lake table already exists for DoctorsUK.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/GCEInmillionconstant2022USD.csv\nDelta Lake table already exists for GCEInmillionconstant2022USD.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/LifeExpectancyBirthAnd60.csv\nDelta Lake table already exists for LifeExpectancyBirthAnd60.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/LifeExpetancyAtBirth.csv\nDelta Lake table already exists for LifeExpetancyAtBirth.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/MonthlyDiagnostics2024_13CLR.csv\nDelta Lake table already exists for MonthlyDiagnostics2024_13CLR.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/NHAIndicators.csv\nDelta Lake table already exists for NHAIndicators.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/PHCExpenditureTrends.csv\nDelta Lake table already exists for PHCExpenditureTrends.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/UKGDPPriceindex2022is100.csv\nDelta Lake table already exists for UKGDPPriceindex2022is100.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/WorldHealthData.csv\nDelta Lake table already exists for WorldHealthData.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/gdppercapitaworldbank.csv\nDelta Lake table already exists for gdppercapitaworldbank.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/ons_nhs_expenditure.csv\nDelta Lake table already exists for ons_nhs_expenditure.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/uknetmigration.csv\nDelta Lake table already exists for uknetmigration.csv, skipping...\nthe file is abfss://healthdata-fs@distributedanalytics.dfs.core.windows.net/data/ukpopulationbyageONS.csv\nDelta Lake table already exists for ukpopulationbyageONS.csv, skipping...\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing of all tables"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import DeltaTable\r\n",
        "\r\n",
        "all_tables = spark.sql(\"SHOW TABLES\").collect()\r\n",
        "delta_tables = []\r\n",
        "\r\n",
        "for table in all_tables:\r\n",
        "    table_full_name = f\"{table.tableName}\"\r\n",
        "    print(table_full_name)\r\n",
        "    if DeltaTable.isDeltaTable(spark, table_full_name):\r\n",
        "        delta_tables.append(table_full_name)\r\n",
        "\r\n",
        "for delta_table in delta_tables:\r\n",
        "    print(delta_table)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ghparkcluster",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "34",
              "normalized_state": "finished",
              "queued_time": "2025-03-17T16:18:40.2823692Z",
              "session_start_time": null,
              "execution_start_time": "2025-03-17T16:24:23.2710293Z",
              "execution_finish_time": "2025-03-17T16:24:47.4511524Z",
              "parent_msg_id": "56426761-9eb6-4ae4-a445-781cb12e0145"
            },
            "text/plain": "StatementMeta(ghparkcluster, 34, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chepercapitausd\ndoctorsuk\nlifeexpectancybirthand60\nnhaindicators\nphcexpendituretrends\nworldhealthdata\ncheaspercentgdp\ncheinmillionconstant2022usd\ngceinmillionconstant2022usd\nmonthlydiagnostics2024_13clr\nukgdppriceindex2022is100\ngdppercapitaworldbank\nuknetmigration\nukpopulationbyageons\nlifeexpetancyatbirth\nons_nhs_expenditure\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}